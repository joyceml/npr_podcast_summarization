{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "187b4c21",
   "metadata": {},
   "source": [
    "### References\n",
    "https://towardsdatascience.com/fine-tuning-the-bart-large-model-for-text-summarization-3c69e4c04582\n",
    "\n",
    "https://github.com/francoisstamant/Fine-tuning-for-text-summarization/blob/main/text_summarization.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a557ac",
   "metadata": {},
   "source": [
    "## Abstractive Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea357fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ohmeow-blurr -q\n",
    "!pip install bert-score -q\n",
    "\n",
    "!pip install torch==1.10.1\n",
    "!pip install fastai==2.5.3\n",
    "!pip install transformers==4.16.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6afa3ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e52b828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1125)>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from fastai.text.all import *\n",
    "from transformers import *\n",
    "from blurr.data.all import *\n",
    "from blurr.modeling.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3121326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22af53e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4180\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('extractive_summaries_v2.csv').drop('Unnamed: 0', axis=1)\n",
    "print(len(df))\n",
    "old_df = pd.read_csv(\"first_1000v2.csv\")\n",
    "df = df[~df[\"title\"].isin(old_df[\"title\"])]\n",
    "df = shuffle(df, random_state = 42).reset_index(drop=True)\n",
    "df = df[['title', 'ext_summaries', 'cleaned_description']]\n",
    "\n",
    "# for gold spotify\n",
    "# df = df[[\"episode name\", \"cleaned_description\", \"ext_summaries\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4511038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>ext_summaries</th>\n",
       "      <th>cleaned_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Bolton Casts Shadow Over Trump Impeachment Defense</td>\n",
       "      <td>We are one week away from the Iowa caucuses. And this Friday, we're going to be live in Des Moines to break down everything you need to know before the big day. Join us by grabbing a ticket at nprpresents.org. OK. Here's the show. Hey there. It's the NPR POLITICS PODCAST. It is16 p.m. on Monday, January 27.I'm The president's legal team continues its defense of President Trump in the Senate impeachment trial. And what Bolton alleges in this book is that President Trump told him in August of 2019 that Trump wanted to continue to hold around $400 million in aid to Ukraine until the Ukrainia...</td>\n",
       "      <td>As President Trump's legal team continues their case for acquittal, a report in The New York Times about an alleged conversation between Trump and Bolton — contained in a draft of the former national security adviser's book manuscript — could change the equation for some senators who are undecided on calling witnesses.And, Joe Biden and Rudy Giuliani were both discussed at length today as the president's lawyers attempt to reframe and undercut the arguments made by Democratic House impeachment managers.Connect:Subscribe to the NPR Politics Podcast here.Email the show at nprpolitics@npr.org...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quick Take: The Battle for Control of the GOP</td>\n",
       "      <td>Hey there. Sam Sanders here popping in to let you know that this episode was recorded before the terrorist attacks in Paris late Friday night. It was a horrible event and one we're going to talk about here on the podcast in an episode soon. But for now, here's this episode. Hey, y'all. It's the NPR POLITICS PODCAST here with a quick take on something really interesting that's been unfolding over the last week or so within the Republican Party. It's a storyline that's been building for some time now. And it was on the main stage last week at the Fox Business Network debate. That night fea...</td>\n",
       "      <td>What does it mean to be a Republican today? The GOP's presidential candidates are fighting to answer that question.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Will A New Georgia Law Restrict Voter Access Or Restore Faith In Elections?</td>\n",
       "      <td>Hi. This is Shawna (ph) from Madison, Wis. And I am getting ready to teach a virtual tap dance class. This podcast was recorded at... 08 p.m. on Monday, March 29. Things may have changed by the time you hear it. All right, enjoy the show. I like hearing that tap sound. I've been curious about taking, like, fitness classes and group things over Zoom. And I have not been able to get myself there, but it sounds like it might be kind of fun. Might sign you up for a tap class - you might be really good at it. Hey there. It's the NPR POLITICS PODCAST.I'm Susan Davis. Republicans say these are n...</td>\n",
       "      <td>A new Georgia law has become the center of the debate over voting rights with President Biden calling it \"Jim Crow in the 21st century.\" Republicans argue the law helps restore faith in the electoral process, but civil rights advocates say it disenfranchises voters of color. Plus, Texas Republicans introduce new bills to restrict voter access.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Encore Interview: Authors Candice Carty-Williams And Angie Thomas</td>\n",
       "      <td>Hi, I'm Anjuli. I'm a producer here on IT'S BEEN A MINUTE. Hi, I'm Alex. I'm an editor on the show. So Alex, let's start by actually telling listeners what it takes to make this show. The process actually begins with pitching. It just begins with throwing ideas out there. And then we actually have to take those ideas and book the people we want on the show. We're taping these interviews with people in studio, sometimes going out to them in the field. And then our producers put their headphones on, and they start cutting this massive thing down. And then editors like Alex on our show are a...</td>\n",
       "      <td>On this special episode, Sam Sanders revisits his 2019 conversations with two writers whose books he loved: Candice Carty-Williams, author of 'Queenie,' and Angie Thomas, author of the books, 'On The Come Up' and 'The Hate U Give.' Candice Carty-Williams' 'Queenie' has been called \"the black 'Bridget Jones' Diary\" and centers around a 25-year-old woman going through the awkwardness of breakups, love and life. Angie Thomas's 'On The Come Up' chronicles the story of a young girl who wants to be a rapper and whose song goes viral in an unexpected way.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NPR Politics Live From Boulder: The Road To 2020</td>\n",
       "      <td>What this says, to me at least, is that after this long and steady rise in the polls, people are finally saying - even if she's not hitting other people, you have other people saying, all right, we're going to hit at her; we see her as a threat. But also, when we sing people out - we don't usually sing it. Right, not 7-foot-tall giants from New York City, but all very, very well-known people, like, people who had very big name recognition namewide, had had large platforms nationwide - Bernie Sanders and Elizabeth Warren, in their own way. I was just recently in South Carolina with Biden ou...</td>\n",
       "      <td>This is a special episode, recorded in front of a live audience at the Boulder Theater in Boulder, Colorado on Friday, September 20th. The cast breaks down everything you need to know about who's running for president, and how they match up next to each other.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                         title  \\\n",
       "0                      John Bolton Casts Shadow Over Trump Impeachment Defense   \n",
       "1                                Quick Take: The Battle for Control of the GOP   \n",
       "2  Will A New Georgia Law Restrict Voter Access Or Restore Faith In Elections?   \n",
       "3            Encore Interview: Authors Candice Carty-Williams And Angie Thomas   \n",
       "4                             NPR Politics Live From Boulder: The Road To 2020   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ext_summaries  \\\n",
       "0   We are one week away from the Iowa caucuses. And this Friday, we're going to be live in Des Moines to break down everything you need to know before the big day. Join us by grabbing a ticket at nprpresents.org. OK. Here's the show. Hey there. It's the NPR POLITICS PODCAST. It is16 p.m. on Monday, January 27.I'm The president's legal team continues its defense of President Trump in the Senate impeachment trial. And what Bolton alleges in this book is that President Trump told him in August of 2019 that Trump wanted to continue to hold around $400 million in aid to Ukraine until the Ukrainia...   \n",
       "1    Hey there. Sam Sanders here popping in to let you know that this episode was recorded before the terrorist attacks in Paris late Friday night. It was a horrible event and one we're going to talk about here on the podcast in an episode soon. But for now, here's this episode. Hey, y'all. It's the NPR POLITICS PODCAST here with a quick take on something really interesting that's been unfolding over the last week or so within the Republican Party. It's a storyline that's been building for some time now. And it was on the main stage last week at the Fox Business Network debate. That night fea...   \n",
       "2   Hi. This is Shawna (ph) from Madison, Wis. And I am getting ready to teach a virtual tap dance class. This podcast was recorded at... 08 p.m. on Monday, March 29. Things may have changed by the time you hear it. All right, enjoy the show. I like hearing that tap sound. I've been curious about taking, like, fitness classes and group things over Zoom. And I have not been able to get myself there, but it sounds like it might be kind of fun. Might sign you up for a tap class - you might be really good at it. Hey there. It's the NPR POLITICS PODCAST.I'm Susan Davis. Republicans say these are n...   \n",
       "3   Hi, I'm Anjuli. I'm a producer here on IT'S BEEN A MINUTE. Hi, I'm Alex. I'm an editor on the show. So Alex, let's start by actually telling listeners what it takes to make this show. The process actually begins with pitching. It just begins with throwing ideas out there. And then we actually have to take those ideas and book the people we want on the show. We're taping these interviews with people in studio, sometimes going out to them in the field. And then our producers put their headphones on, and they start cutting this massive thing down. And then editors like Alex on our show are a...   \n",
       "4  What this says, to me at least, is that after this long and steady rise in the polls, people are finally saying - even if she's not hitting other people, you have other people saying, all right, we're going to hit at her; we see her as a threat. But also, when we sing people out - we don't usually sing it. Right, not 7-foot-tall giants from New York City, but all very, very well-known people, like, people who had very big name recognition namewide, had had large platforms nationwide - Bernie Sanders and Elizabeth Warren, in their own way. I was just recently in South Carolina with Biden ou...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       cleaned_description  \n",
       "0  As President Trump's legal team continues their case for acquittal, a report in The New York Times about an alleged conversation between Trump and Bolton — contained in a draft of the former national security adviser's book manuscript — could change the equation for some senators who are undecided on calling witnesses.And, Joe Biden and Rudy Giuliani were both discussed at length today as the president's lawyers attempt to reframe and undercut the arguments made by Democratic House impeachment managers.Connect:Subscribe to the NPR Politics Podcast here.Email the show at nprpolitics@npr.org...  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     What does it mean to be a Republican today? The GOP's presidential candidates are fighting to answer that question.   \n",
       "2                                                                                                                                                                                                                                                               A new Georgia law has become the center of the debate over voting rights with President Biden calling it \"Jim Crow in the 21st century.\" Republicans argue the law helps restore faith in the electoral process, but civil rights advocates say it disenfranchises voters of color. Plus, Texas Republicans introduce new bills to restrict voter access.   \n",
       "3                                               On this special episode, Sam Sanders revisits his 2019 conversations with two writers whose books he loved: Candice Carty-Williams, author of 'Queenie,' and Angie Thomas, author of the books, 'On The Come Up' and 'The Hate U Give.' Candice Carty-Williams' 'Queenie' has been called \"the black 'Bridget Jones' Diary\" and centers around a 25-year-old woman going through the awkwardness of breakups, love and life. Angie Thomas's 'On The Come Up' chronicles the story of a young girl who wants to be a rapper and whose song goes viral in an unexpected way.  \n",
       "4                                                                                                                                                                                                                                                                                                                                                    This is a special episode, recorded in front of a live audience at the Boulder Theater in Boulder, Colorado on Friday, September 20th. The cast breaks down everything you need to know about who's running for president, and how they match up next to each other.   "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65786b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = int(round(4180 * 0.9)) - 1000\n",
    "\n",
    "train_df = df.iloc[:train_len].reset_index(drop=True)\n",
    "test_df = df.iloc[train_len:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34d71248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>ext_summaries</th>\n",
       "      <th>cleaned_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Weekly Roundup: December 18th</td>\n",
       "      <td>And then as it provides updates to its customers, which include U.S. government agencies, big businesses and others, they are ingesting the malware that you seeded a couple of those steps away. The list of affected government agencies at this point in time - we know that the Commerce Department, the Department of Homeland Security, the Department of Defense, the Treasury Department, Postal Service, Energy Department, National Institutes of Health have all been affected in some way, shape or form. Or will this be one of the things for which the administration doesn't act until we have the a...</td>\n",
       "      <td>A major hack of federal government computer systems appears to be state-sponsored espionage. And we talk through Joe Biden's latest cabinet picks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amid Many Global Crises Biden Calls For Togetherness In First U.N. Address</td>\n",
       "      <td>Hi. This is Christina (ph) from Austin, Texas. And right now I'm just sitting on my couch with my dog, George (ph). George is very tired. But it's because he and I passed our therapy team test. He's now officially a therapy animal. This podcast was recorded at... 33 p.m. Eastern Time on Tuesday, September 21. Things may have changed by the time you listen to it. But hopefully George wakes up from this nap soon. All right. President Biden delivered his first address to the United Nations General Assembly today, and it comes just weeks after the chaotic withdrawal from Afghanistan. But I ju...</td>\n",
       "      <td>In his first address to the United Nations General Assembly, President Biden emphasized the importance of global cooperation to combat the coronavirus and climate change. And he not so subtly critiqued China and authoritarianism.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How The Rat Blew Up</td>\n",
       "      <td>This is the moment where we're supposed to ask you to show your support for PLANET MONEY by making a donation, but that's not exactly fun. So we made this little promo instead. Do you have economic questions rattling around in your head, driving you nuts, keeping you awake at night? You've tried shouting them out a window. (As character) Why are oil prices negative? But nobody answers. You've tried bringing them up during pillow talk. (As character) Have you ever heard of an inverted yield curve? (As character) Honey, I have a headache. (As character) Maybe another night. Even Siri doesn'...</td>\n",
       "      <td>Unions have been putting giant inflatable rats in front of businesses for years. Now businesses are trying to deflate them, in court. | Subscribe to our weekly newsletter here.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Madness Of March</td>\n",
       "      <td>The quest for the crown is underway.() Jim, we're going to see some of the best long-armed quick athletes that there is in college basketball. But Ed and his teammates were student athletes. They wanted to file a lawsuit against the NCAA because they said that the NCAA's control of college athletes and how much they could earn violated laws around fair compensation. So the NCAA has thousands of member colleges and universities and hundreds of thousands of student athletes. According to a poll conducted last fall by The Washington Post and ABC News, nearly 2 out of 3 Americans, in fact, thi...</td>\n",
       "      <td>The NCAA men's basketball tournament is going on right now and will bring in hundreds of millions of dollars in revenue. The coaches and commissioners who benefit are overwhelmingly white. The players on the court are MOSTLY black. So what, if anything, are those players owed?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How To Choose A Health Insurance Plan</td>\n",
       "      <td>When open enrollment for health insurance rolls around, usually in the fall, HR departments and local brokers try to make it fun - balloons, kazoos. Happy open enrollment, everyone - so exciting. Are you having fun yet? No? For real, picking health insurance is work. Before we dive in, let's talk about why health insurance is worth having - in other words, why it's worthwhile to put in all the effort to find a plan and enroll. To begin with, you can think about health insurance like your local fire department. I'm a research professor at Georgetown University, and I co-direct something c...</td>\n",
       "      <td>Picking health insurance takes a lot of work. It's not a one-size-fits-all type of situation, and there are a lot of confusing terms that come up every year. You also need to consider your general health and finances. So how can you make sense of it all? That's where we come in. In this episode of Life Kit, we'll have experts guide you on where to look for coverage, how to narrow down plans — and how to get trustworthy help if you need it.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        title  \\\n",
       "0                                               Weekly Roundup: December 18th   \n",
       "1  Amid Many Global Crises Biden Calls For Togetherness In First U.N. Address   \n",
       "2                                                         How The Rat Blew Up   \n",
       "3                                                        The Madness Of March   \n",
       "4                                       How To Choose A Health Insurance Plan   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ext_summaries  \\\n",
       "0  And then as it provides updates to its customers, which include U.S. government agencies, big businesses and others, they are ingesting the malware that you seeded a couple of those steps away. The list of affected government agencies at this point in time - we know that the Commerce Department, the Department of Homeland Security, the Department of Defense, the Treasury Department, Postal Service, Energy Department, National Institutes of Health have all been affected in some way, shape or form. Or will this be one of the things for which the administration doesn't act until we have the a...   \n",
       "1   Hi. This is Christina (ph) from Austin, Texas. And right now I'm just sitting on my couch with my dog, George (ph). George is very tired. But it's because he and I passed our therapy team test. He's now officially a therapy animal. This podcast was recorded at... 33 p.m. Eastern Time on Tuesday, September 21. Things may have changed by the time you listen to it. But hopefully George wakes up from this nap soon. All right. President Biden delivered his first address to the United Nations General Assembly today, and it comes just weeks after the chaotic withdrawal from Afghanistan. But I ju...   \n",
       "2   This is the moment where we're supposed to ask you to show your support for PLANET MONEY by making a donation, but that's not exactly fun. So we made this little promo instead. Do you have economic questions rattling around in your head, driving you nuts, keeping you awake at night? You've tried shouting them out a window. (As character) Why are oil prices negative? But nobody answers. You've tried bringing them up during pillow talk. (As character) Have you ever heard of an inverted yield curve? (As character) Honey, I have a headache. (As character) Maybe another night. Even Siri doesn'...   \n",
       "3  The quest for the crown is underway.() Jim, we're going to see some of the best long-armed quick athletes that there is in college basketball. But Ed and his teammates were student athletes. They wanted to file a lawsuit against the NCAA because they said that the NCAA's control of college athletes and how much they could earn violated laws around fair compensation. So the NCAA has thousands of member colleges and universities and hundreds of thousands of student athletes. According to a poll conducted last fall by The Washington Post and ABC News, nearly 2 out of 3 Americans, in fact, thi...   \n",
       "4    When open enrollment for health insurance rolls around, usually in the fall, HR departments and local brokers try to make it fun - balloons, kazoos. Happy open enrollment, everyone - so exciting. Are you having fun yet? No? For real, picking health insurance is work. Before we dive in, let's talk about why health insurance is worth having - in other words, why it's worthwhile to put in all the effort to find a plan and enroll. To begin with, you can think about health insurance like your local fire department. I'm a research professor at Georgetown University, and I co-direct something c...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                           cleaned_description  \n",
       "0                                                                                                                                                                                                                                                                                                           A major hack of federal government computer systems appears to be state-sponsored espionage. And we talk through Joe Biden's latest cabinet picks.  \n",
       "1                                                                                                                                                                                                                       In his first address to the United Nations General Assembly, President Biden emphasized the importance of global cooperation to combat the coronavirus and climate change. And he not so subtly critiqued China and authoritarianism.   \n",
       "2                                                                                                                                                                                                                                                                             Unions have been putting giant inflatable rats in front of businesses for years. Now businesses are trying to deflate them, in court. | Subscribe to our weekly newsletter here.  \n",
       "3                                                                                                                                                                        The NCAA men's basketball tournament is going on right now and will bring in hundreds of millions of dollars in revenue. The coaches and commissioners who benefit are overwhelmingly white. The players on the court are MOSTLY black. So what, if anything, are those players owed?  \n",
       "4  Picking health insurance takes a lot of work. It's not a one-size-fits-all type of situation, and there are a lot of confusing terms that come up every year. You also need to consider your general health and finances. So how can you make sense of it all? That's where we come in. In this episode of Life Kit, we'll have experts guide you on where to look for coverage, how to narrow down plans — and how to get trustworthy help if you need it.  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3221970",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name = \"facebook/bart-large-cnn\"\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name, \n",
    "                                                                  model_cls=BartForConditionalGeneration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25247e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bart'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea06ac51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartConfig {\n",
       "  \"_name_or_path\": \"facebook/bart-large-cnn\",\n",
       "  \"_num_labels\": 3,\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"add_final_layer_norm\": false,\n",
       "  \"architectures\": [\n",
       "    \"BartForConditionalGeneration\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classif_dropout\": 0.0,\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_model\": 1024,\n",
       "  \"decoder_attention_heads\": 16,\n",
       "  \"decoder_ffn_dim\": 4096,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 12,\n",
       "  \"decoder_start_token_id\": 2,\n",
       "  \"dropout\": 0.1,\n",
       "  \"early_stopping\": true,\n",
       "  \"encoder_attention_heads\": 16,\n",
       "  \"encoder_ffn_dim\": 4096,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 12,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"force_bos_token_to_be_generated\": true,\n",
       "  \"forced_bos_token_id\": 0,\n",
       "  \"forced_eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2\n",
       "  },\n",
       "  \"length_penalty\": 2.0,\n",
       "  \"max_length\": 142,\n",
       "  \"max_position_embeddings\": 1024,\n",
       "  \"min_length\": 56,\n",
       "  \"model_type\": \"bart\",\n",
       "  \"no_repeat_ngram_size\": 3,\n",
       "  \"normalize_before\": false,\n",
       "  \"num_beams\": 4,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"prefix\": \" \",\n",
       "  \"scale_embedding\": false,\n",
       "  \"task_specific_params\": {\n",
       "    \"summarization\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"length_penalty\": 2.0,\n",
       "      \"max_length\": 142,\n",
       "      \"min_length\": 56,\n",
       "      \"no_repeat_ngram_size\": 3,\n",
       "      \"num_beams\": 4\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.16.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50264\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e3deeba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='facebook/bart-large-cnn', vocab_size=50265, model_max_len=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e5d1ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50264, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e212efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_batch_tfm = HF_Seq2SeqBeforeBatchTransform(hf_arch, hf_config, hf_tokenizer, hf_model, task='summarization',\n",
    "text_gen_kwargs={\n",
    "    # vals are controlling for padding, do not need\n",
    "#     'max_length': 248,\n",
    "#  'min_length': 56,\n",
    " 'do_sample': False,\n",
    "    # change early stopping to False\n",
    " 'early_stopping': False,\n",
    " 'num_beams': 4,\n",
    " 'temperature': 1.0,\n",
    " 'top_k': 50,\n",
    " 'top_p': 1.0,\n",
    " 'repetition_penalty': 1.0,\n",
    " 'bad_words_ids': None,\n",
    " 'bos_token_id': 0,\n",
    " 'pad_token_id': 1,\n",
    " 'eos_token_id': 2,\n",
    " 'length_penalty': 2.0,\n",
    " 'no_repeat_ngram_size': 3,\n",
    " 'encoder_no_repeat_ngram_size': 0,\n",
    " 'num_return_sequences': 1,\n",
    " 'decoder_start_token_id': 2,\n",
    " 'use_cache': True,\n",
    " 'num_beam_groups': 1,\n",
    " 'diversity_penalty': 0.0,\n",
    " 'output_attentions': False,\n",
    " 'output_hidden_states': False,\n",
    " 'output_scores': False,\n",
    " 'return_dict_in_generate': False,\n",
    " 'forced_bos_token_id': 0,\n",
    " 'forced_eos_token_id': 2,\n",
    " 'remove_invalid_values': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cb0746d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare data for training\n",
    "blocks = (HF_Seq2SeqBlock(before_batch_tfm=hf_batch_tfm), noop)\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, get_x=ColReader('ext_summaries'), get_y=ColReader('cleaned_description'), splitter=RandomSplitter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22634092",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(test_df, bs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7108b5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define performance metrics\n",
    "seq2seq_metrics = {\n",
    "        'rouge': {\n",
    "            'compute_kwargs': { 'rouge_types': [\"rouge1\", \"rouge2\", \"rougeL\"], 'use_stemmer': True },\n",
    "            'returns': [\"rouge1\", \"rouge2\", \"rougeL\"]\n",
    "        },\n",
    "        'bertscore': {\n",
    "            'compute_kwargs': { 'lang': 'fr' },\n",
    "            'returns': [\"precision\", \"recall\", \"f1\"]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3efd46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    }
   ],
   "source": [
    "model = HF_BaseModelWrapper(hf_model)\n",
    "learn_cbs = [HF_BaseModelCallback]\n",
    "fit_cbs = [HF_Seq2SeqMetricsCallback(custom_metrics=seq2seq_metrics)]\n",
    "\n",
    "learn = Learner(dls, \n",
    "                model,\n",
    "                opt_func=ranger,\n",
    "                loss_func=CrossEntropyLossFlat(),\n",
    "                cbs=learn_cbs,\n",
    "                splitter=partial(seq2seq_splitter, arch=hf_arch)).to_fp16()\n",
    "\n",
    "learn.create_opt() \n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ab79965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR FINE TUNING\n",
    "# start = time.time()\n",
    "# learn.fit_one_cycle(1, lr_max=3e-5, cbs=fit_cbs)\n",
    "# end = time.time()\n",
    "# print(end-start)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# FOR RELOADING\n",
    "new_learn = Learner(dls, \n",
    "                model,\n",
    "                opt_func=ranger,\n",
    "                loss_func=CrossEntropyLossFlat(),\n",
    "                cbs=learn_cbs,\n",
    "                splitter=partial(seq2seq_splitter, arch=hf_arch)).to_fp16()\n",
    "new_learn  = new_learn.load(file = \"fine_tuned_bart_cnn_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ab6f722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.save(\"fine_tuned_bart_cnn_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f5d676",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7918bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 14.15987777709961\n",
      "1 24.91643190383911\n",
      "2 22.961649894714355\n",
      "3 27.153764963150024\n",
      "4 24.191830158233643\n",
      "5 24.804927110671997\n",
      "6 23.679725170135498\n",
      "7 28.837836027145386\n",
      "8 16.029541015625\n",
      "9 14.92687702178955\n",
      "10 17.352323055267334\n",
      "11 21.58385396003723\n",
      "12 13.471438884735107\n",
      "13 19.09633994102478\n",
      "14 15.627184867858887\n",
      "15 25.00234889984131\n",
      "16 15.092352151870728\n",
      "17 13.286089897155762\n",
      "18 21.596368074417114\n",
      "19 11.3197181224823\n",
      "20 16.546256065368652\n",
      "21 26.357810258865356\n",
      "22 15.847020149230957\n",
      "23 30.381231784820557\n",
      "24 29.112615823745728\n",
      "25 33.568498849868774\n",
      "26 17.10931897163391\n",
      "27 20.420526027679443\n",
      "28 21.57655096054077\n",
      "29 24.779188871383667\n",
      "30 31.12147617340088\n",
      "31 17.743190050125122\n",
      "32 29.525331020355225\n",
      "33 19.130318880081177\n",
      "34 21.868350982666016\n",
      "35 33.795284032821655\n",
      "36 36.129814863204956\n",
      "37 17.740372896194458\n",
      "38 21.93385601043701\n",
      "39 19.41888999938965\n",
      "40 16.36072278022766\n",
      "41 18.999150276184082\n",
      "42 18.26339888572693\n",
      "43 25.61358904838562\n",
      "44 23.944570779800415\n",
      "45 14.139671802520752\n",
      "46 22.170112133026123\n",
      "47 21.935500144958496\n",
      "48 21.411136865615845\n",
      "49 11.779259204864502\n",
      "50 12.934120893478394\n",
      "51 15.264389753341675\n",
      "52 18.989686012268066\n",
      "53 19.560144186019897\n",
      "54 26.511791944503784\n",
      "55 25.26337480545044\n",
      "56 29.886147022247314\n",
      "57 18.911978006362915\n",
      "58 26.714223861694336\n",
      "59 24.439810037612915\n",
      "60 16.336180925369263\n",
      "61 20.84199810028076\n",
      "62 18.120474815368652\n",
      "63 25.037926197052002\n",
      "64 26.286031246185303\n",
      "65 24.451410055160522\n",
      "66 22.398730993270874\n",
      "67 14.604340076446533\n",
      "68 24.0389142036438\n",
      "69 12.861443996429443\n",
      "70 26.075325965881348\n",
      "71 11.211806058883667\n",
      "72 13.760837078094482\n",
      "73 24.875868797302246\n",
      "74 24.642822980880737\n",
      "75 23.57409977912903\n",
      "76 16.461534023284912\n",
      "77 25.305273056030273\n",
      "78 22.757318019866943\n",
      "79 24.177581071853638\n",
      "80 21.40129804611206\n",
      "81 25.5622980594635\n",
      "82 22.387446880340576\n",
      "83 25.77914071083069\n",
      "84 23.547368049621582\n",
      "85 25.85593295097351\n",
      "86 23.724180936813354\n",
      "87 15.16900110244751\n",
      "88 28.800985097885132\n",
      "89 24.65143918991089\n",
      "90 24.336939096450806\n",
      "91 11.669691324234009\n",
      "92 14.93714690208435\n",
      "93 22.806036949157715\n",
      "94 20.630099058151245\n",
      "95 13.672972679138184\n",
      "96 19.088063955307007\n",
      "97 22.356317043304443\n",
      "98 25.51599407196045\n",
      "99 14.661096811294556\n",
      "100 14.585126161575317\n",
      "101 12.510214805603027\n",
      "102 29.011346340179443\n",
      "103 15.960707187652588\n",
      "104 20.66825270652771\n",
      "105 14.6946120262146\n",
      "106 19.29423999786377\n",
      "107 18.02376103401184\n",
      "108 24.310847997665405\n",
      "109 14.143710851669312\n",
      "110 20.50096082687378\n",
      "111 16.075629711151123\n",
      "112 16.794055938720703\n",
      "113 12.516830921173096\n",
      "114 16.890624046325684\n",
      "115 25.97261905670166\n",
      "116 23.40396499633789\n",
      "117 16.332496881484985\n",
      "118 27.18045735359192\n",
      "119 16.053601026535034\n",
      "120 15.39557409286499\n",
      "121 26.347396850585938\n",
      "122 23.92444109916687\n",
      "123 10.621654033660889\n",
      "124 12.239256143569946\n",
      "125 12.68847107887268\n",
      "126 19.18191409111023\n",
      "127 25.367938995361328\n",
      "128 18.811403036117554\n",
      "129 14.972348928451538\n",
      "130 24.375567197799683\n",
      "131 26.00933599472046\n",
      "132 22.42281699180603\n",
      "133 24.864017963409424\n",
      "134 15.8768470287323\n",
      "135 13.490110158920288\n",
      "136 15.172816276550293\n",
      "137 24.765660047531128\n",
      "138 19.807384252548218\n",
      "139 23.99901580810547\n",
      "140 12.427613973617554\n",
      "141 17.236906051635742\n",
      "142 25.411555767059326\n",
      "143 15.376998901367188\n",
      "144 26.931957006454468\n",
      "145 25.730005025863647\n",
      "146 25.602633237838745\n",
      "147 11.533103942871094\n",
      "148 25.309278964996338\n",
      "149 23.026002168655396\n",
      "150 13.74628210067749\n",
      "151 18.88661789894104\n",
      "152 23.470698356628418\n",
      "153 23.809552907943726\n",
      "154 23.15806293487549\n",
      "155 12.971333026885986\n",
      "156 12.810565710067749\n",
      "157 26.917301177978516\n",
      "158 13.864473104476929\n",
      "159 25.262418031692505\n",
      "160 24.447211027145386\n",
      "161 26.007998943328857\n",
      "162 24.68604016304016\n",
      "163 15.270407915115356\n",
      "164 18.837063789367676\n",
      "165 23.195592880249023\n",
      "166 20.678457975387573\n",
      "167 12.057074069976807\n",
      "168 13.819310903549194\n",
      "169 25.410507917404175\n",
      "170 14.329469203948975\n",
      "171 10.703414916992188\n",
      "172 17.389474868774414\n",
      "173 24.994292974472046\n",
      "174 20.409857988357544\n",
      "175 23.75977373123169\n",
      "176 15.498769998550415\n",
      "177 24.805509090423584\n",
      "178 15.515615940093994\n",
      "179 25.119099140167236\n",
      "180 13.278382778167725\n",
      "181 10.69897723197937\n",
      "182 27.406861066818237\n",
      "183 27.073078870773315\n",
      "184 20.189457893371582\n",
      "185 18.91177797317505\n",
      "186 21.033672332763672\n",
      "187 25.007503747940063\n",
      "188 14.459986925125122\n",
      "189 24.659966945648193\n",
      "190 25.899179935455322\n",
      "191 13.673451900482178\n",
      "192 15.385597944259644\n",
      "193 25.277333974838257\n",
      "194 13.235224723815918\n",
      "195 24.341130018234253\n",
      "196 26.271331787109375\n",
      "197 18.209984064102173\n",
      "198 13.742639780044556\n",
      "199 19.555021047592163\n",
      "200 13.15177607536316\n",
      "201 23.85856318473816\n",
      "202 24.26475191116333\n",
      "203 19.71869993209839\n",
      "204 23.61174511909485\n",
      "205 15.829219818115234\n",
      "206 27.60712695121765\n",
      "207 11.670594215393066\n",
      "208 16.77629828453064\n",
      "209 21.27224326133728\n",
      "210 13.495540857315063\n",
      "211 25.864389181137085\n",
      "212 16.874422073364258\n",
      "213 17.24402093887329\n",
      "214 25.206479787826538\n",
      "215 20.107109785079956\n",
      "216 31.931103944778442\n",
      "217 14.433889865875244\n",
      "218 15.391897201538086\n",
      "219 14.152427911758423\n",
      "220 22.951648712158203\n",
      "221 17.915521144866943\n",
      "222 29.501587867736816\n",
      "223 15.535737991333008\n",
      "224 13.237733125686646\n",
      "225 21.647537231445312\n",
      "226 19.264349937438965\n",
      "227 17.900115966796875\n",
      "228 31.16817617416382\n",
      "229 36.02136492729187\n",
      "230 27.77965784072876\n",
      "231 25.512542963027954\n",
      "232 36.07259488105774\n",
      "233 35.495145320892334\n",
      "234 27.67683482170105\n",
      "235 27.777657985687256\n",
      "236 16.33559823036194\n",
      "237 38.640358209609985\n",
      "238 19.355105876922607\n",
      "239 36.32412099838257\n",
      "240 19.001343965530396\n",
      "241 18.6464421749115\n",
      "242 30.775362014770508\n",
      "243 28.460614919662476\n",
      "244 33.39184498786926\n",
      "245 25.86389994621277\n",
      "246 15.125235080718994\n",
      "247 36.53365111351013\n",
      "248 30.09005904197693\n",
      "249 17.34494709968567\n",
      "250 20.12300181388855\n",
      "251 26.463207960128784\n",
      "252 33.471861600875854\n",
      "253 19.33397912979126\n",
      "254 33.19324278831482\n",
      "255 24.68603801727295\n",
      "256 33.49871611595154\n",
      "257 33.33145308494568\n",
      "258 33.21862196922302\n",
      "259 19.513735055923462\n",
      "260 22.432535648345947\n",
      "261 17.029584169387817\n",
      "262 19.58600902557373\n",
      "263 28.01163387298584\n",
      "264 38.04589486122131\n",
      "265 18.39381718635559\n",
      "266 31.31964373588562\n",
      "267 14.01975679397583\n",
      "268 18.76793122291565\n",
      "269 16.314682006835938\n",
      "270 16.28309917449951\n",
      "271 23.89253282546997\n",
      "272 24.767027139663696\n",
      "273 14.205602884292603\n",
      "274 15.752528190612793\n",
      "275 26.357378005981445\n",
      "276 14.24120807647705\n",
      "277 12.447905778884888\n",
      "278 26.981953144073486\n",
      "279 11.548689842224121\n",
      "280 17.415063858032227\n",
      "281 14.92646598815918\n",
      "282 19.95800518989563\n",
      "283 12.847877025604248\n",
      "284 15.911549091339111\n",
      "285 26.553062200546265\n",
      "286 20.836740255355835\n",
      "287 14.562010765075684\n",
      "288 25.318341970443726\n",
      "289 12.57399296760559\n",
      "290 24.62855100631714\n",
      "291 26.470129013061523\n",
      "292 13.091295003890991\n",
      "293 25.72604203224182\n",
      "294 10.959719896316528\n",
      "295 16.800477027893066\n",
      "296 23.483431816101074\n",
      "297 28.501699924468994\n",
      "298 11.204303979873657\n",
      "299 15.397459745407104\n",
      "300 27.14576005935669\n",
      "301 23.871538877487183\n",
      "302 23.521435260772705\n",
      "303 21.173355102539062\n",
      "304 25.478321075439453\n",
      "305 26.552905321121216\n",
      "306 25.19621467590332\n",
      "307 23.45662522315979\n",
      "308 26.801453113555908\n",
      "309 25.442008018493652\n",
      "310 20.912847757339478\n",
      "311 11.822354793548584\n",
      "312 15.387260913848877\n",
      "313 26.269699811935425\n",
      "314 27.9913010597229\n",
      "315 18.25667715072632\n",
      "316 23.429508209228516\n",
      "317 28.168729066848755\n",
      "318 25.909638166427612\n",
      "319 14.442528009414673\n",
      "320 20.931222915649414\n",
      "321 28.89433002471924\n",
      "322 13.061255931854248\n",
      "323 18.454777002334595\n",
      "324 18.99102282524109\n",
      "325 26.60218596458435\n",
      "326 27.224478006362915\n",
      "327 15.867372751235962\n",
      "328 28.208783864974976\n",
      "329 16.66177797317505\n",
      "330 18.882678031921387\n",
      "331 28.4274742603302\n",
      "332 20.274247884750366\n",
      "333 14.499893188476562\n",
      "334 22.88747215270996\n",
      "335 19.380279064178467\n",
      "336 22.80282497406006\n",
      "337 27.372315883636475\n",
      "338 20.733822107315063\n",
      "339 14.91917085647583\n",
      "340 18.87545895576477\n",
      "341 25.139058113098145\n",
      "342 18.511908054351807\n",
      "343 27.10112190246582\n",
      "344 13.165606260299683\n",
      "345 22.44216012954712\n",
      "346 25.586798906326294\n",
      "347 25.59058713912964\n",
      "348 25.770453214645386\n",
      "349 27.44862389564514\n",
      "350 28.17554998397827\n",
      "351 27.66332721710205\n",
      "352 28.93757700920105\n",
      "353 19.32110905647278\n",
      "354 24.27010726928711\n",
      "355 14.843477010726929\n",
      "356 19.13395094871521\n",
      "357 26.842182159423828\n",
      "358 27.690380811691284\n",
      "359 27.188616037368774\n",
      "360 12.658124208450317\n",
      "361 15.688476800918579\n",
      "362 29.935724020004272\n",
      "363 28.431419134140015\n",
      "364 14.886707782745361\n",
      "365 19.16425085067749\n",
      "366 31.10725712776184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367 17.282117128372192\n",
      "368 26.628830909729004\n",
      "369 18.54355525970459\n",
      "370 27.005949020385742\n",
      "371 25.220686674118042\n",
      "372 17.539305925369263\n",
      "373 27.97062397003174\n",
      "374 23.22268581390381\n",
      "375 22.644584894180298\n",
      "376 18.861665964126587\n",
      "377 13.459535121917725\n",
      "378 26.378538131713867\n",
      "379 25.004909992218018\n",
      "380 26.115046739578247\n",
      "381 20.242048025131226\n",
      "382 21.735896110534668\n",
      "383 19.050585985183716\n",
      "384 10.934141874313354\n",
      "385 23.80578374862671\n",
      "386 31.867028951644897\n",
      "387 15.50243091583252\n",
      "388 18.537184953689575\n",
      "389 25.926053047180176\n",
      "390 19.24666714668274\n",
      "391 22.50014591217041\n",
      "392 27.078269958496094\n",
      "393 19.068174123764038\n",
      "394 16.322293996810913\n",
      "395 29.82699990272522\n",
      "396 28.40213894844055\n",
      "397 17.659571886062622\n",
      "398 26.775118112564087\n",
      "399 20.169328212738037\n",
      "400 25.076535940170288\n",
      "401 14.333399057388306\n",
      "402 18.089038133621216\n",
      "403 20.298736095428467\n",
      "404 21.24873113632202\n",
      "405 18.27119016647339\n",
      "406 11.799903869628906\n",
      "407 16.188091039657593\n",
      "408 23.639997959136963\n",
      "409 25.534321308135986\n",
      "410 27.443011045455933\n",
      "411 24.00640892982483\n",
      "412 21.080607891082764\n",
      "413 17.85040593147278\n",
      "414 23.278458833694458\n",
      "415 14.16978931427002\n"
     ]
    }
   ],
   "source": [
    "train_rouge1s, train_rouge2s, train_rougeLs = [], [], []\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2','rougeLsum'], use_stemmer=True)\n",
    "train_outputs = []\n",
    "\n",
    "# train_outputs = [learn.blurr_generate(i, early_stopping=False, num_return_sequences=1) for i in train_df['ext_summaries']]\n",
    "for i in range(len(test_df['ext_summaries'])):\n",
    "    start = time.time()\n",
    "    train_outputs += [new_learn.blurr_generate(test_df['ext_summaries'][i], \n",
    "                                           early_stopping=False, num_return_sequences=1)]\n",
    "    scores = scorer.score(train_outputs[-1][0], test_df['cleaned_description'][i])\n",
    "    train_rouge1s += [scores['rouge1'][2]]\n",
    "    train_rouge2s += [scores['rouge2'][2]]\n",
    "    train_rougeLs += [scores['rougeLsum'][2]]\n",
    "    end = time.time()\n",
    "    print(i, end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a565112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58c1e2e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "curr_df = test_df.copy()\n",
    "curr_df[\"final_summary\"] = train_outputs\n",
    "curr_df[\"rouge1 final\"] = train_rouge1s\n",
    "curr_df[\"rouge2 final\"] = train_rouge2s\n",
    "curr_df[\"rougeLsum final\"] = train_rougeLs\n",
    "# curr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ac0d6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first3500 = pd.read_csv(\"first_3500.csv\", index_col = 0)\n",
    "# first4000 = pd.concat([first3500, curr_df])\n",
    "# first4000.to_csv(\"all_train_final.csv\")\n",
    "\n",
    "curr_df.to_csv(\"test_final_summaries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7761b70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROUGE 1:  0.29926070878928346\n",
      "Test ROUGE 2:  0.10360578490359791\n",
      "Test ROUGE LSUM:  0.193881724958022\n"
     ]
    }
   ],
   "source": [
    "print(\"Test ROUGE 1: \", np.mean(curr_df[\"rouge1 final\"]))\n",
    "print(\"Test ROUGE 2: \", np.mean(curr_df[\"rouge2 final\"]))\n",
    "print(\"Test ROUGE LSUM: \", np.mean(curr_df[\"rougeLsum final\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407e47b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df[\"final_summary\"] = train_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deba91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# test_outputs = [learn.blurr_generate(i, early_stopping=False, num_return_sequences=1) for i in test_df['ext_summaries']]\n",
    "# end = time.time()\n",
    "# print(end-start)\n",
    "# test_df[\"final_summary\"] = test_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c585d1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rouge_score import rouge_scorer\n",
    "\n",
    "# train_rouge1s, train_rouge2s, train_rougeLs = [], [], []\n",
    "# scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2','rougeLsum'], use_stemmer=True)\n",
    "\n",
    "# for i in range(len(train_outputs)):\n",
    "#     scores = scorer.score(train_outputs[i][0], df['cleaned_description'][i])\n",
    "#     train_rouge1s += [scores['rouge1'][2]]\n",
    "#     train_rouge2s += [scores['rouge2'][2]]\n",
    "#     train_rougeLs += [scores['rougeLsum'][2]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6bb1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_df.to_csv(\"first_100.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370bb9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(train_rouge1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e0e033",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.mean(train_rouge2s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7782f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(train_rougeLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7e0bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rouge1s, test_rouge2s, test_rougeLs = [], [], []\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2','rougeLsum'], use_stemmer=True)\n",
    "\n",
    "for i in range(len(test_outputs)):\n",
    "    scores = scorer.score(test_outputs[i][0], df['cleaned_description'][i])\n",
    "    test_rouge1s += [scores['rouge1'][2]]\n",
    "    test_rouge2s += [scores['rouge2'][2]]\n",
    "    test_rougeLs += [scores['rougeLsum'][2]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbac3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(test_rouge1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8927376",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.mean(test_rouge2s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddbfae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(test_rougeLs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10842445",
   "metadata": {},
   "source": [
    "# Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c289b265",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_learn = Learner(dls, \n",
    "                model,\n",
    "                opt_func=ranger,\n",
    "                loss_func=CrossEntropyLossFlat(),\n",
    "                cbs=learn_cbs,\n",
    "                splitter=partial(seq2seq_splitter, arch=hf_arch)).to_fp16()\n",
    "new_learn  = new_learn.load(file = \"learner_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96bbb45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outputs = [new_learn.blurr_generate(i, early_stopping=False, num_return_sequences=1) for i in df['ext_summaries']]\n",
    "\n",
    "for idx, o in enumerate(outputs):\n",
    "    print(f'=== Prediction {idx+1} ===\\n{o}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1cbd2d",
   "metadata": {},
   "source": [
    "# ROUGE Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ab1b267",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_df = pd.read_csv(\"all_train_final.csv\", index_col = 0)\n",
    "final_test_df = pd.read_csv(\"test_final.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01287bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rouge1s, train_rouge2s, train_rougeLs = [], [], []\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2','rougeLsum'], use_stemmer=True)\n",
    "\n",
    "for i in range(len(final_test_df)):\n",
    "    row = final_test_df.iloc[i]\n",
    "    scores = scorer.score(row[\"final_summary\"][2:-2], row[\"cleaned_description\"])\n",
    "    train_rouge1s += [scores['rouge1'][2]]\n",
    "    train_rouge2s += [scores['rouge2'][2]]\n",
    "    train_rougeLs += [scores['rougeLsum'][2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f723ebcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test ROUGE 1: \", np.mean(train_rouge1s))\n",
    "print(\"Test ROUGE 2: \", np.mean(train_rouge2s))\n",
    "print(\"Test ROUGE LSUM: \", np.mean(train_rougeLs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "656f7b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROUGE 1:  0.3057937642173362\n",
      "Train ROUGE 2:  0.10367278842471121\n",
      "Train ROUGE LSUM:  0.1952444678881282\n"
     ]
    }
   ],
   "source": [
    "print(\"Train ROUGE 1: \", np.mean(final_train_df[\"rouge1 final\"]))\n",
    "print(\"Train ROUGE 2: \", np.mean(final_train_df[\"rouge2 final\"]))\n",
    "print(\"Train ROUGE LSUM: \", np.mean(final_train_df[\"rougeLsum final\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fcfc027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROUGE 1:  0.31189312505436156\n",
      "Test ROUGE 2:  0.11083795388258275\n",
      "Test ROUGE LSUM:  0.20375157203321168\n"
     ]
    }
   ],
   "source": [
    "print(\"Test ROUGE 1: \", np.mean(final_test_df[\"rouge1 final\"]))\n",
    "print(\"Test ROUGE 2: \", np.mean(final_test_df[\"rouge2 final\"]))\n",
    "print(\"Test ROUGE LSUM: \", np.mean(final_test_df[\"rougeLsum final\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "686bd2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold ROUGE 1:  0.23434786987766865\n",
      "Gold ROUGE 2:  0.0450038137181914\n",
      "Gold ROUGE LSUM:  0.13869516285085218\n"
     ]
    }
   ],
   "source": [
    "gold_df = pd.read_csv(\"gold_final.csv\", index_col = 0)\n",
    "print(\"Gold ROUGE 1: \", np.mean(gold_df[\"rouge1 final\"]))\n",
    "print(\"Gold ROUGE 2: \", np.mean(gold_df[\"rouge2 final\"]))\n",
    "print(\"Gold ROUGE LSUM: \", np.mean(gold_df[\"rougeLsum final\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3216acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df = pd.read_csv(\"all_summaries.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4ec3301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rouge 1</th>\n",
       "      <th>rouge 2</th>\n",
       "      <th>rouge L sum</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Changes To Texas Voting Rules Worry Older Voters And Those With Disabilities</td>\n",
       "      <td>Score(precision=0.2972972972972973, recall=0.2894736842105263, fmeasure=0.29333333333333333)</td>\n",
       "      <td>Score(precision=0.0273972602739726, recall=0.02666666666666667, fmeasure=0.027027027027027025)</td>\n",
       "      <td>Score(precision=0.14864864864864866, recall=0.14473684210526316, fmeasure=0.14666666666666667)</td>\n",
       "      <td>This was the first Election Day since sweeping election changes in the state that drastically change how a lot of people cast ballots. Only a couple of counties took advantage of unlimited polling hours and drive-through voting. Voting rights advocates say there is quite obviously going to be discriminatory effects from creating new limits on this type of voting. There was also some concern about provisions in the law that give poll-watchers broader authority.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>At The State Of The Union, Biden Wooed Moderates And Slammed Putin</td>\n",
       "      <td>Score(precision=0.3448275862068966, recall=0.16129032258064516, fmeasure=0.2197802197802198)</td>\n",
       "      <td>Score(precision=0.03508771929824561, recall=0.016260162601626018, fmeasure=0.022222222222222223)</td>\n",
       "      <td>Score(precision=0.1724137931034483, recall=0.08064516129032258, fmeasure=0.1098901098901099)</td>\n",
       "      <td>President Biden delivered his first State of the Union address on Tuesday. CNN's John Sutter says it was a very traditional American leadership speech. Sutter: \"Get back to normal was sort of his message, which I thought was interesting\" Biden's poll numbers among independent voters, they're in the toilet. He is profoundly unpopular with people who don't identify with either party. He didn't talk more about voting rights. He did not talk about explicitly passing police reform legislation through Congress. I think the relationships between lawmakers have been particularly nasty. There was a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Awake Not Woke\": How Republicans Are Defining Their Party in 2022</td>\n",
       "      <td>Score(precision=0.20754716981132076, recall=0.2972972972972973, fmeasure=0.24444444444444446)</td>\n",
       "      <td>Score(precision=0.019230769230769232, recall=0.027777777777777776, fmeasure=0.02272727272727273)</td>\n",
       "      <td>Score(precision=0.09433962264150944, recall=0.13513513513513514, fmeasure=0.11111111111111112)</td>\n",
       "      <td>Domenico Montanaro: CPAC is now the Trump show. He says the former president, American president, has been essentially praising Putin. Domenico: It's not necessarily predictive of anything except for the people who are in that room.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ketanji Brown Jackson Is The First Black Woman Nominated To The Supreme Court</td>\n",
       "      <td>Score(precision=0.4791666666666667, recall=0.19491525423728814, fmeasure=0.27710843373493976)</td>\n",
       "      <td>Score(precision=0.10638297872340426, recall=0.042735042735042736, fmeasure=0.06097560975609756)</td>\n",
       "      <td>Score(precision=0.3125, recall=0.1271186440677966, fmeasure=0.18072289156626506)</td>\n",
       "      <td>Teresa: She's only the second generation in her family who had anybody go to college and that her ancestors were most likely, on both sides, her mother's side and her father's side, slaves. Domenico: A new poll out today is not looking good for President Biden. 56% say his first year in office was a failure, with just 39% saying it was a success. We've seen a decline in suburban voters by double digits in their approval of President Biden in just the last two months. Biden is teaching himself Arabic - showoff. He's also apparently teaching himself how to vote. Biden is in the running to be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US Responds To Russian Invasion Of Ukraine With Stronger Sanctions</td>\n",
       "      <td>Score(precision=0.4444444444444444, recall=0.21739130434782608, fmeasure=0.29197080291970806)</td>\n",
       "      <td>Score(precision=0.045454545454545456, recall=0.02197802197802198, fmeasure=0.029629629629629627)</td>\n",
       "      <td>Score(precision=0.2, recall=0.09782608695652174, fmeasure=0.13138686131386862)</td>\n",
       "      <td>Vladimir Putin ignored months of warnings from the U.S., Europe and allies around the world that an invasion will provoke severe consequences for Russia and its people. It's really it's also a challenge by Putin to NATO countries, and that includes the United States. The threat of sanctions did not deter Vladimir Putin from going into Ukraine. Russia doesn't - it's not a manufacturing state, you know, a great manufacturing state. If Putin wanted to retaliate on something like this, certainly he could cut off gas to Europe.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                           title  \\\n",
       "0   Changes To Texas Voting Rules Worry Older Voters And Those With Disabilities   \n",
       "1             At The State Of The Union, Biden Wooed Moderates And Slammed Putin   \n",
       "2             \"Awake Not Woke\": How Republicans Are Defining Their Party in 2022   \n",
       "3  Ketanji Brown Jackson Is The First Black Woman Nominated To The Supreme Court   \n",
       "4             US Responds To Russian Invasion Of Ukraine With Stronger Sanctions   \n",
       "\n",
       "                                                                                         rouge 1  \\\n",
       "0   Score(precision=0.2972972972972973, recall=0.2894736842105263, fmeasure=0.29333333333333333)   \n",
       "1   Score(precision=0.3448275862068966, recall=0.16129032258064516, fmeasure=0.2197802197802198)   \n",
       "2  Score(precision=0.20754716981132076, recall=0.2972972972972973, fmeasure=0.24444444444444446)   \n",
       "3  Score(precision=0.4791666666666667, recall=0.19491525423728814, fmeasure=0.27710843373493976)   \n",
       "4  Score(precision=0.4444444444444444, recall=0.21739130434782608, fmeasure=0.29197080291970806)   \n",
       "\n",
       "                                                                                            rouge 2  \\\n",
       "0    Score(precision=0.0273972602739726, recall=0.02666666666666667, fmeasure=0.027027027027027025)   \n",
       "1  Score(precision=0.03508771929824561, recall=0.016260162601626018, fmeasure=0.022222222222222223)   \n",
       "2  Score(precision=0.019230769230769232, recall=0.027777777777777776, fmeasure=0.02272727272727273)   \n",
       "3   Score(precision=0.10638297872340426, recall=0.042735042735042736, fmeasure=0.06097560975609756)   \n",
       "4  Score(precision=0.045454545454545456, recall=0.02197802197802198, fmeasure=0.029629629629629627)   \n",
       "\n",
       "                                                                                      rouge L sum  \\\n",
       "0  Score(precision=0.14864864864864866, recall=0.14473684210526316, fmeasure=0.14666666666666667)   \n",
       "1    Score(precision=0.1724137931034483, recall=0.08064516129032258, fmeasure=0.1098901098901099)   \n",
       "2  Score(precision=0.09433962264150944, recall=0.13513513513513514, fmeasure=0.11111111111111112)   \n",
       "3                Score(precision=0.3125, recall=0.1271186440677966, fmeasure=0.18072289156626506)   \n",
       "4                  Score(precision=0.2, recall=0.09782608695652174, fmeasure=0.13138686131386862)   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   summary  \n",
       "0                                                                                                                                         This was the first Election Day since sweeping election changes in the state that drastically change how a lot of people cast ballots. Only a couple of counties took advantage of unlimited polling hours and drive-through voting. Voting rights advocates say there is quite obviously going to be discriminatory effects from creating new limits on this type of voting. There was also some concern about provisions in the law that give poll-watchers broader authority.  \n",
       "1  President Biden delivered his first State of the Union address on Tuesday. CNN's John Sutter says it was a very traditional American leadership speech. Sutter: \"Get back to normal was sort of his message, which I thought was interesting\" Biden's poll numbers among independent voters, they're in the toilet. He is profoundly unpopular with people who don't identify with either party. He didn't talk more about voting rights. He did not talk about explicitly passing police reform legislation through Congress. I think the relationships between lawmakers have been particularly nasty. There was a...  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                 Domenico Montanaro: CPAC is now the Trump show. He says the former president, American president, has been essentially praising Putin. Domenico: It's not necessarily predictive of anything except for the people who are in that room.  \n",
       "3  Teresa: She's only the second generation in her family who had anybody go to college and that her ancestors were most likely, on both sides, her mother's side and her father's side, slaves. Domenico: A new poll out today is not looking good for President Biden. 56% say his first year in office was a failure, with just 39% saying it was a success. We've seen a decline in suburban voters by double digits in their approval of President Biden in just the last two months. Biden is teaching himself Arabic - showoff. He's also apparently teaching himself how to vote. Biden is in the running to be...  \n",
       "4                                                                         Vladimir Putin ignored months of warnings from the U.S., Europe and allies around the world that an invasion will provoke severe consequences for Russia and its people. It's really it's also a challenge by Putin to NATO countries, and that includes the United States. The threat of sanctions did not deter Vladimir Putin from going into Ukraine. Russia doesn't - it's not a manufacturing state, you know, a great manufacturing state. If Putin wanted to retaliate on something like this, certainly he could cut off gas to Europe.  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9227783",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
